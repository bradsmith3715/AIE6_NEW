{"questions": {}, "relevant_contexts": {}, "corpus": {"e996f28e-5740-40f1-b569-0d3ef0e4c676": "The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\nThe pace of change doesn\u2019t help either. In just the past month we\u2019ve seen general availability of live interfaces where you can point your phone\u2019s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven\u2019t even tried that yet.\nGiven the ongoing (and potential) impact on society that this technology has, I don\u2019t think the size of this gap is healthy. I\u2019d like to see a lot more effort put into improving this.\nLLMs need better criticism", "ac4191fc-a5ef-4226-97ec-2945b29ebea1": "A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that \u201cLLMs are useful\u201d can be enough to kick off a huge fight.\nI get it. There are plenty of reasons to dislike this technology\u2014the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people\u2019s jobs.\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.", "583eb1d2-fc78-4a27-9ca3-823dacdb13be": "I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n(If you still don\u2019t think there are any good applications at all I\u2019m not sure why you made it to this point in the article!)", "06ce07b1-7bc4-49ae-84e2-eabe7a09f384": "I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\nThose of us who understand this stuff have a duty to help everyone else figure it out.\nEverything tagged \u201cllms\u201d on my blog in 2024\nBecause I undoubtedly missed a whole bunch of things, here\u2019s every long-form post I wrote in 2024 that I tagged with llms:", "8ec9587d-db73-4a9f-b813-e309582a67b7": "January\n\n7th: It\u2019s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\n\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\n\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit\u2014a new plugin for LLM\n\n\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)", "4f84beae-81ed-4202-8b4a-e6f240cee35f": "17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\n\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in \u201c4o\u201d mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don\u2019t remember everything you say\n\n\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\n\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024", "ae572781-2324-4823-a31a-ea00ee491270": "19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\n\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude\u2019s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\n\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI\u2019s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code", "50e19a87-2879-4b7d-b6b2-11b2c6100931": "29th: NotebookLM\u2019s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\n\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let\u2019s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic\u2019s new Computer Use capability", "c5b49dd7-135b-4e2d-bddd-294a61be2936": "24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W\u0336e\u0336e\u0336k\u0336n\u0336o\u0336t\u0336e\u0336s\u0336  Monthnotes for October\n\n\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD\u2014tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat\u2014Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\n\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js", "5e4d6e3b-b739-44d9-af21-36d3dc6dffdf": "7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it\u2019s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash \u201cThinking mode\u201d\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI\u2014\u201cEarly evals for OpenAI o3\u201d\n\n24th: Trying out QvQ\u2014Qwen\u2019s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n\n\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)", "1acfc8ea-7b72-4c43-a5cb-4c75f8c45521": "Posted 31st December 2024 at 6:07 pm \u00b7 Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter\n\n\nMore recent articles\n\nQwen 3 offers a case study in how to effectively release a model - 29th April 2025\nWatching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025\nExploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025\n\n\n \n\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. \n\n\n\n            google\n            360\n\n\n            ai\n            1251", "344aa347-5d1f-4743-9df7-b723386124f8": "openai\n            287\n\n\n            generative-ai\n            1078\n\n\n            local-llms\n            112\n\n\n            llms\n            1066\n\n\n            anthropic\n            135\n\n\n            gemini\n            80\n\n\n            meta\n            31\n\n\n            llm-reasoning\n            43\n\n\n            long-context\n            15\n\n\n            ai-energy-usage\n            5\n\nNext: Ending a year long posting streak\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\n\n \n \n\n\nColophon\n\u00a9\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025"}}